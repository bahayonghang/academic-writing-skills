# LaTeX å­¦æœ¯å†™ä½œ Claude Code Skills å®Œæ•´æ–¹æ¡ˆ

## ç›®å½•
1. [æ–¹æ¡ˆæ¦‚è¿°](#æ–¹æ¡ˆæ¦‚è¿°)
2. [ç›®å½•ç»“æ„](#ç›®å½•ç»“æ„)
3. [Skill 1: è‹±æ–‡å­¦æœ¯è®ºæ–‡](#skill-1-è‹±æ–‡å­¦æœ¯è®ºæ–‡)
4. [Skill 2: ä¸­æ–‡åšå£«è®ºæ–‡](#skill-2-ä¸­æ–‡åšå£«è®ºæ–‡)
5. [æ ¸å¿ƒè„šæœ¬](#æ ¸å¿ƒè„šæœ¬)
6. [é…å¥—æ–‡æ¡£](#é…å¥—æ–‡æ¡£)
7. [Hook é…ç½®](#hook-é…ç½®)
8. [éƒ¨ç½²æŒ‡å—](#éƒ¨ç½²æŒ‡å—)

---

## æ–¹æ¡ˆæ¦‚è¿°

### è®¾è®¡ç†å¿µ
- **åˆ†ç¦»å…³æ³¨ç‚¹**: è‹±æ–‡è®ºæ–‡å’Œä¸­æ–‡è®ºæ–‡åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹ Skills
- **å››å±‚æ¶æ„**: L0 åªè¯»å®¡æŸ¥ â†’ L1 å»ºè®® â†’ L2 Diffç”Ÿæˆ â†’ L3 è‡ªåŠ¨åº”ç”¨(å¯é€‰)
- **å®‰å…¨ä¼˜å…ˆ**: é»˜è®¤åªè¯»ï¼Œä¿®æ”¹ä»¥æ³¨é‡Šå½¢å¼è¾“å‡º
- **å·¥å…·å¢å¼º**: é›†æˆ chktex, latexmk ç­‰æˆç†Ÿå·¥å…·
- **æ¸è¿›åŠ è½½**: æŒ‰éœ€åŠ è½½è¯¦ç»†æ–‡æ¡£ï¼ŒèŠ‚çœä¸Šä¸‹æ–‡

### æ ¸å¿ƒåŠŸèƒ½å¯¹æ¯”

| åŠŸèƒ½ | è‹±æ–‡è®ºæ–‡ | ä¸­æ–‡è®ºæ–‡ | å®ç°æ–¹å¼ |
|------|---------|---------|---------|
| æ ¼å¼å®¡æŸ¥ | âœ… | âœ… | chktex + latexmk |
| è¯­æ³•åˆ†æ | âœ… (è‹±æ–‡) | âœ… (ä¸­æ–‡) | LLM + LanguageTool |
| é•¿éš¾å¥æ‹†è§£ | âœ… | âœ… | LLM åˆ†æ |
| è¡¨è¿°é‡æ„ | âœ… | âœ… | LLM + æœ¯è¯­ä¿æŠ¤ |
| ç»“æ„æ˜ å°„ | å•æ–‡ä»¶ | â­ å¤šæ–‡ä»¶ | map_structure.py |
| æœŸåˆŠé€‚é… | â­ IEEE/ACMç­‰ | - | VENUES.md |
| å­¦æ ¡æ¨¡æ¿ | - | â­ æ¸…å/åŒ—å¤§ç­‰ | UNIVERSITIES/ |
| å›½æ ‡æ£€æŸ¥ | - | â­ GB/T 7714 | GB_STANDARD.md |

---

## ç›®å½•ç»“æ„

```
.claude/
â”œâ”€â”€ skills/
â”‚   â”œâ”€â”€ latex-paper-en/              # Skill 1: è‹±æ–‡å­¦æœ¯è®ºæ–‡
â”‚   â”‚   â”œâ”€â”€ SKILL.md                 # æ ¸å¿ƒæŠ€èƒ½å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”‚   â”œâ”€â”€ STYLE_GUIDE.md       # å†™ä½œé£æ ¼æŒ‡å—
â”‚   â”‚   â”‚   â”œâ”€â”€ VENUES.md            # æœŸåˆŠ/ä¼šè®®è§„åˆ™
â”‚   â”‚   â”‚   â”œâ”€â”€ COMMON_ERRORS.md     # Chinglish é”™è¯¯åº“
â”‚   â”‚   â”‚   â””â”€â”€ FORBIDDEN_CHANGES.md # ç¦æ­¢ä¿®æ”¹æœ¯è¯­
â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â”œâ”€â”€ check_format.sh      # chktex åŒ…è£…å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_prose.py     # æå–çº¯æ–‡æœ¬
â”‚   â”‚   â”‚   â”œâ”€â”€ analyze_sentence.py  # é•¿éš¾å¥åˆ†æ
â”‚   â”‚   â”‚   â”œâ”€â”€ verify_bib.py        # BibTeX æ£€æŸ¥
â”‚   â”‚   â”‚   â””â”€â”€ make_diff.py         # ç”Ÿæˆ Diff
â”‚   â”‚   â””â”€â”€ templates/
â”‚   â”‚       â”œâ”€â”€ diff_comment.txt     # Diff æ¨¡æ¿
â”‚   â”‚       â””â”€â”€ report_template.md   # æŠ¥å‘Šæ¨¡æ¿
â”‚   â”‚
â”‚   â””â”€â”€ latex-thesis-zh/             # Skill 2: ä¸­æ–‡åšå£«è®ºæ–‡
â”‚       â”œâ”€â”€ SKILL.md                 # æ ¸å¿ƒæŠ€èƒ½å®šä¹‰
â”‚       â”œâ”€â”€ docs/
â”‚       â”‚   â”œâ”€â”€ STRUCTURE_GUIDE.md   # ç»“æ„å®Œæ•´æ€§æŒ‡å—
â”‚       â”‚   â”œâ”€â”€ GB_STANDARD.md       # å›½æ ‡æ ¼å¼è§„èŒƒ
â”‚       â”‚   â”œâ”€â”€ ACADEMIC_STYLE_ZH.md # ä¸­æ–‡å­¦æœ¯è§„èŒƒ
â”‚       â”‚   â”œâ”€â”€ FORBIDDEN_CHANGES.md # ç¦æ­¢ä¿®æ”¹æœ¯è¯­
â”‚       â”‚   â””â”€â”€ UNIVERSITIES/        # å­¦æ ¡æ¨¡æ¿åº“
â”‚       â”‚       â”œâ”€â”€ tsinghua.md      # æ¸…åå¤§å­¦
â”‚       â”‚       â”œâ”€â”€ pku.md           # åŒ—äº¬å¤§å­¦
â”‚       â”‚       â””â”€â”€ generic.md       # é€šç”¨æ¨¡æ¿
â”‚       â”œâ”€â”€ scripts/
â”‚       â”‚   â”œâ”€â”€ map_structure.py     # â­ è®ºæ–‡ç»“æ„æ˜ å°„
â”‚       â”‚   â”œâ”€â”€ check_format.sh      # chktexï¼ˆä¸­æ–‡ï¼‰
â”‚       â”‚   â”œâ”€â”€ extract_prose.py     # ä¸­æ–‡æ–‡æœ¬æå–
â”‚       â”‚   â”œâ”€â”€ check_consistency.py # æœ¯è¯­ä¸€è‡´æ€§
â”‚       â”‚   â”œâ”€â”€ verify_structure.py  # ç»“æ„éªŒè¯
â”‚       â”‚   â””â”€â”€ make_diff.py         # ç”Ÿæˆä¸­æ–‡ Diff
â”‚       â””â”€â”€ templates/
â”‚           â”œâ”€â”€ structure_tree.json  # ç»“æ„æ¨¡æ¿
â”‚           â””â”€â”€ report_template_zh.md
â”‚
â””â”€â”€ hooks/
    â””â”€â”€ post-tool-use.yaml           # â­ è´¨é‡é—¨ç¦ Hook
```

---

## Skill 1: è‹±æ–‡å­¦æœ¯è®ºæ–‡

### SKILL.md

```markdown
---
name: latex-paper-en
description: LaTeX academic paper assistant for English conference/journal papers. Handles format checking (chktex), grammar analysis, complex sentence decomposition, and expression restructuring. Use when working with .tex files for papers, or when user mentions "paper review", "grammar check", "improve writing", or specific venues like IEEE/ACM/Springer.
allowed-tools: Read, Grep, Glob, Bash
metadata:
  version: "1.0.0"
  author: "Academic Writing Team"
  category: "LaTeX Academic Writing"
  target_document: "English Research Papers (6-20 pages)"
  processing_mode: "whole_file"
---

# LaTeX Academic Paper Assistant (English)

## Core Philosophy

<critical_rules>
**NEVER violate these rules:**
1. NEVER modify content inside `\cite{}`, `\ref{}`, `\label{}`, `\equation`, `\align`, or any math environment
2. NEVER fabricate or hallucinate bibliography entries - only read from existing .bib files
3. NEVER change domain-specific terminology without explicit permission (see FORBIDDEN_CHANGES.md)
4. NEVER apply changes directly - always output in diff-comment format first
5. ALWAYS run chktex before analyzing text content
</critical_rules>

## Operating Mode Detection

When user requests help with LaTeX writing, automatically detect mode:

**Mode A: Paper (6-20 pages)**
- Indicators: "conference", "journal", "short paper", single `.tex` file
- Strategy: Whole file processing
- Focus: Conciseness, clarity, active voice

**Mode B: Long Paper (>20 pages)**
- Indicators: Multiple chapters, large file
- Strategy: Section-by-section processing
- Focus: Same as Mode A but process incrementally

## Workflow (4-Layer Approach)

### Layer 0: Pre-flight Check (MANDATORY)

```bash
# Step 1: Run chktex (if available)
bash scripts/check_format.sh main.tex

# Step 2: Verify BibTeX integrity
python3 scripts/verify_bib.py references.bib

# Step 3: Map file structure (if multi-file)
find . -name "*.tex" -type f
```

**Output**: Format health report (PASS/WARN/FAIL)

### Layer 1: Grammar & Style Analysis (Read-Only)

```bash
# Extract pure prose (skip math/citations)
python3 scripts/extract_prose.py main.tex > prose_only.txt

# Analyze sentence complexity
python3 scripts/analyze_sentence.py prose_only.txt
```

**Focus Areas**:
1. **Grammar Issues**
   - Subject-verb agreement
   - Article usage (a/an/the)
   - Tense consistency (past for methods, present for results)
   
2. **Chinglish Detection** (see COMMON_ERRORS.md)
   - âŒ "Obviously, as we all know"
   - âœ… "Notably, prior work has established"
   
3. **Weak Verbs Replacement**
   - make â†’ construct/generate/produce
   - do â†’ perform/conduct/execute
   - get â†’ obtain/achieve/derive

### Layer 2: Complex Sentence Decomposition

**Trigger**: Sentences >50 words or >3 subordinate clauses

**Analysis Format**:
```latex
% LONG SENTENCE DETECTED (Line 45, 67 words)
% Original:
% The proposed method, which leverages deep learning techniques to extract features from raw data, can effectively improve the performance of downstream tasks when compared to traditional approaches that rely on hand-crafted features.

% Core Clause:
% The proposed method can effectively improve performance.

% Subordinate Structure:
% - [Relative] which leverages deep learning techniques
% - [Purpose] to extract features from raw data  
% - [Temporal] when compared to traditional approaches
% - [Relative] that rely on hand-crafted features

% Suggested Rewrite (Conservative):
% The proposed method leverages deep learning to extract features from raw data.
% This approach effectively improves performance in downstream tasks.
% Compared to traditional approaches relying on hand-crafted features, our method demonstrates superior results.

% Suggested Rewrite (Aggressive - Academic Style):
% We propose a deep learning-based feature extraction method.
% Experimental results demonstrate that this approach outperforms traditional hand-crafted feature methods in downstream tasks.
```

### Layer 3: Expression Restructuring

**Output Format** (LaTeX-safe diff):
```latex
% SUGGESTION (Line 23): Improve academic tone
% Before:
% We use machine learning to get better results.
% 
% After (Reason: Replace weak verbs + formal structure):
We employ machine learning techniques to achieve superior performance.
%
% Changes:
% - "use" â†’ "employ" (more formal)
% - "get" â†’ "achieve" (academic)
% - "better results" â†’ "superior performance" (precise)
```

## Style Guidelines by Venue

Load venue-specific rules from `docs/VENUES.md`:
- IEEE: Active voice for contributions, past tense for methods
- ACM: Present tense for general truths, structured abstract required
- Springer: Figure captions below, table captions above
- NeurIPS: Concise (8 pages), no appendix in main paper

## Output Structure

```markdown
# LaTeX Paper Review Report

## ğŸ“‹ Executive Summary
- Status: âœ… READY / âš ï¸ NEEDS REVISION / âŒ MAJOR ISSUES
- Compilation: [chktex results]
- Grammar Issues: X found
- Long Sentences: Y found
- Suggested Changes: Z locations

## ğŸ”§ Critical Issues (Must Fix)
[Issues that prevent compilation or violate format requirements]

## ğŸ“ Grammar & Style Improvements
[Categorized by severity: High/Medium/Low]

## ğŸ“ Long Sentence Analysis
[Each long sentence with decomposition]

## ğŸ¨ Expression Refinements
[Academic tone improvements with rationale]

## ğŸ“š BibTeX Verification
[Missing fields, formatting issues]

## âœ… Next Steps
[Concrete action items]
```

## Tool Usage

```bash
# Format checking (Layer 0)
chktex -q -v0 main.tex

# Word count check
texcount -sum -1 main.tex

# Find TODOs
grep -n "TODO\|FIXME\|XXX" *.tex

# Verify all citations have entries
python3 scripts/verify_bib.py main.tex references.bib
```

## Safety Mechanisms

1. **Read-only by default**: Only suggest, never auto-apply
2. **Diff-comment format**: Changes embedded as LaTeX comments
3. **Forbidden terms protection**: Load from FORBIDDEN_CHANGES.md
4. **Citation integrity**: Verify all `\cite{}` exist in .bib
5. **Math preservation**: Never parse content between `$...$` or `\[...\]`

## Error Handling

```python
# If chktex not available
if not command_exists("chktex"):
    print("âš ï¸ chktex not found. Install with: sudo apt-get install chktex")
    print("ğŸ“ Falling back to LLM-based format check (less precise)")
    # Continue with degraded mode
```

## Progressive Disclosure

Start with high-level summary. Only load detailed files when needed:
- `STYLE_GUIDE.md`: When analyzing writing style
- `VENUES.md`: When user mentions specific conference/journal
- `COMMON_ERRORS.md`: When analyzing grammar
- `FORBIDDEN_CHANGES.md`: Before any term modification

## References

For detailed guidance:
- Writing style: [STYLE_GUIDE.md](docs/STYLE_GUIDE.md)
- Common errors: [COMMON_ERRORS.md](docs/COMMON_ERRORS.md)
- Venue requirements: [VENUES.md](docs/VENUES.md)
- Protected terms: [FORBIDDEN_CHANGES.md](docs/FORBIDDEN_CHANGES.md)
```

---

## Skill 2: ä¸­æ–‡åšå£«è®ºæ–‡

### SKILL.md

```markdown
---
name: latex-thesis-zh
description: ä¸­æ–‡åšå£«/ç¡•å£«å­¦ä½è®ºæ–‡ LaTeX åŠ©æ‰‹ã€‚æ”¯æŒç»“æ„å®Œæ•´æ€§æ£€æŸ¥ã€å›½æ ‡æ ¼å¼å®¡æŸ¥ï¼ˆGB/T 7714ï¼‰ã€ä¸­æ–‡å­¦æœ¯è¡¨è¾¾è§„èŒƒã€æœ¯è¯­ä¸€è‡´æ€§åˆ†æã€‚é€‚ç”¨äºå¤šæ–‡ä»¶è®ºæ–‡é¡¹ç›®ï¼ˆmain.tex + ç« èŠ‚æ–‡ä»¶ï¼‰ï¼Œè‡ªåŠ¨è¯†åˆ«å­¦æ ¡æ¨¡æ¿ï¼ˆæ¸…å/åŒ—å¤§ç­‰ï¼‰ã€‚å½“ç”¨æˆ·æåˆ°"åšå£«è®ºæ–‡"ã€"å­¦ä½è®ºæ–‡"ã€"æ¯•ä¸šè®ºæ–‡"ã€"è®ºæ–‡å®¡æŸ¥"æ—¶ä½¿ç”¨ã€‚
allowed-tools: Read, Grep, Glob, Bash
metadata:
  version: "1.0.0"
  author: "å­¦æœ¯å†™ä½œå›¢é˜Ÿ"
  category: "LaTeX å­¦æœ¯å†™ä½œ"
  target_document: "ä¸­æ–‡åšå£«/ç¡•å£«å­¦ä½è®ºæ–‡ï¼ˆ100-300é¡µï¼‰"
  processing_mode: "chapter_by_chapter"
  supported_templates: "thu-thesis, pkuthss, ustcthesis, fduthesis"
---

# LaTeX åšå£«è®ºæ–‡åŠ©æ‰‹ï¼ˆä¸­æ–‡ï¼‰

## æ ¸å¿ƒåŸåˆ™

<critical_rules>
**ç»å¯¹ç¦æ­¢è¿åä»¥ä¸‹è§„åˆ™ï¼š**
1. ç»ä¸ä¿®æ”¹ `\cite{}`ã€`\ref{}`ã€`\label{}`ã€å…¬å¼ç¯å¢ƒå†…çš„å†…å®¹
2. ç»ä¸å‡­ç©ºæé€ å‚è€ƒæ–‡çŒ®æ¡ç›® - åªèƒ½è¯»å–ç°æœ‰ .bib æ–‡ä»¶
3. ç»ä¸åœ¨æœªç»è®¸å¯çš„æƒ…å†µä¸‹ä¿®æ”¹ä¸“ä¸šæœ¯è¯­ï¼ˆè§ FORBIDDEN_CHANGES.mdï¼‰
4. ç»ä¸ç›´æ¥åº”ç”¨ä¿®æ”¹ - å§‹ç»ˆå…ˆä»¥æ³¨é‡Šå½¢å¼è¾“å‡º diff
5. å¿…é¡»å…ˆè¿è¡Œ map_structure.py äº†è§£è®ºæ–‡ç»“æ„ï¼Œå†å¼€å§‹åˆ†æ
6. å¿…é¡»å…ˆè¿è¡Œ chktex æ ¼å¼æ£€æŸ¥ï¼Œå†è¿›è¡Œå†…å®¹åˆ†æ
</critical_rules>

## å·¥ä½œæ¨¡å¼ï¼šåšå£«è®ºæ–‡ï¼ˆ100-300 é¡µï¼‰

**ç‰¹å¾è¯†åˆ«**ï¼š
- å…³é”®è¯ï¼š"åšå£«è®ºæ–‡"ã€"å­¦ä½è®ºæ–‡"ã€"dissertation"ã€"thesis"
- æ–‡ä»¶ç»“æ„ï¼š`main.tex` + `data/chap01.tex` ç­‰å¤šä¸ªç« èŠ‚æ–‡ä»¶
- æ¨¡æ¿ç±»å‹ï¼š`\documentclass{thuthesis}` æˆ– `\documentclass[doctor]{pkuthss}` ç­‰

**å¤„ç†ç­–ç•¥**ï¼šæŒ‰ç« èŠ‚é€ä¸ªå¤„ç†ï¼Œé¿å…ä¸Šä¸‹æ–‡ä¸¢å¤±

## å·¥ä½œæµç¨‹ï¼ˆ5å±‚æ–¹æ³•ï¼‰

### Layer 0: ç»“æ„æ˜ å°„ï¼ˆå¼ºåˆ¶æ‰§è¡Œï¼‰

```bash
# â­ æ ¸å¿ƒè„šæœ¬ï¼šç”Ÿæˆè®ºæ–‡æ–‡ä»¶æ ‘
python3 scripts/map_structure.py main.tex

# è¾“å‡ºç¤ºä¾‹ï¼š
# main.tex
# â”œâ”€â”€ data/cover.tex (å°é¢)
# â”œâ”€â”€ data/abstract.tex (ä¸­è‹±æ–‡æ‘˜è¦)
# â”œâ”€â”€ data/chap01.tex (ç¬¬ä¸€ç« ï¼šç»ªè®º)
# â”œâ”€â”€ data/chap02.tex (ç¬¬äºŒç« ï¼šç›¸å…³å·¥ä½œ)
# â”œâ”€â”€ data/chap03.tex (ç¬¬ä¸‰ç« ï¼šæ–¹æ³•)
# â””â”€â”€ ref/refs.bib (å‚è€ƒæ–‡çŒ®)

# æ£€æµ‹å­¦æ ¡æ¨¡æ¿
grep "documentclass" main.tex | python3 scripts/detect_template.py
```

**è¾“å‡º**ï¼šæ–‡ä»¶ç»“æ„æ ‘ + æ¨¡æ¿ç±»å‹ + å»ºè®®å¤„ç†é¡ºåº

### Layer 1: ç»“æ„å®Œæ•´æ€§æ£€æŸ¥

æ ¹æ® `docs/STRUCTURE_GUIDE.md` æ£€æŸ¥ï¼š

**å‰ç½®éƒ¨åˆ†**ï¼ˆå¿…é¡»é¡¹ï¼‰ï¼š
- âœ… å°é¢ï¼ˆcoverï¼‰
- âœ… åŸåˆ›æ€§å£°æ˜å’Œä½¿ç”¨æˆæƒï¼ˆdeclarationï¼‰
- âœ… ä¸­æ–‡æ‘˜è¦ + å…³é”®è¯
- âœ… è‹±æ–‡æ‘˜è¦ + Keywords
- âœ… ç›®å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
- âš ï¸ ç¬¦å·å¯¹ç…§è¡¨ï¼ˆå¯é€‰ä½†æ¨èï¼‰

**æ­£æ–‡éƒ¨åˆ†**ï¼ˆç»“æ„è§„èŒƒï¼‰ï¼š
- âœ… ç¬¬1ç« ï¼šç»ªè®ºï¼ˆç ”ç©¶èƒŒæ™¯ã€æ„ä¹‰ã€é—®é¢˜ã€è´¡çŒ®ã€ç»“æ„ï¼‰
- âœ… ç¬¬2ç« ï¼šç›¸å…³å·¥ä½œ/æ–‡çŒ®ç»¼è¿°
- âœ… ç¬¬3-Nç« ï¼šæ ¸å¿ƒå†…å®¹ï¼ˆæ–¹æ³•ã€å®éªŒã€åˆ†æï¼‰
- âœ… æœ€åä¸€ç« ï¼šæ€»ç»“ä¸å±•æœ›

**åç½®éƒ¨åˆ†**ï¼ˆå¿…é¡»é¡¹ï¼‰ï¼š
- âœ… å‚è€ƒæ–‡çŒ®
- âœ… è‡´è°¢
- âœ… æ”»è¯»å­¦ä½æœŸé—´å‘è¡¨çš„å­¦æœ¯è®ºæ–‡ç›®å½•
- âš ï¸ é™„å½•ï¼ˆå¦‚æœ‰ï¼‰

**æ£€æŸ¥è„šæœ¬**ï¼š
```bash
python3 scripts/verify_structure.py main.tex --template tsinghua
```

### Layer 2: å›½æ ‡æ ¼å¼å®¡æŸ¥ï¼ˆGB/T 7714-2015/2025ï¼‰

**é‡ç‚¹æ£€æŸ¥é¡¹**ï¼ˆè§ `docs/GB_STANDARD.md`ï¼‰ï¼š

1. **å‚è€ƒæ–‡çŒ®æ ¼å¼**
   ```bash
   # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å›½æ ‡æ ·å¼
   grep "bibliographystyle\|biblatex" main.tex
   
   # æ¨èï¼šbiblatex-gb7714-2015
   # æ£€æŸ¥ .bib æ–‡ä»¶å®Œæ•´æ€§
   python3 scripts/verify_bib.py ref/refs.bib --standard gb7714
   ```

2. **å›¾è¡¨é¢˜æ³¨è§„èŒƒ**
   - å›¾é¢˜ï¼šä½äºå›¾ä¸‹æ–¹ï¼Œå®‹ä½“äº”å·
   - è¡¨é¢˜ï¼šä½äºè¡¨ä¸Šæ–¹ï¼Œå®‹ä½“äº”å·
   - æ ¼å¼ï¼š`å›¾ 3.1` æˆ– `å›¾3-1`ï¼ˆå–å†³äºæ¨¡æ¿ï¼‰

3. **å…¬å¼ç¼–å·**
   - æ ¼å¼ï¼š`(3.1)` è¡¨ç¤ºç¬¬3ç« ç¬¬1ä¸ªå…¬å¼
   - å¯¹é½ï¼šå³å¯¹é½

4. **ç« èŠ‚æ ‡é¢˜å±‚çº§**
   - ä¸€çº§æ ‡é¢˜ï¼ˆç« ï¼‰ï¼šé»‘ä½“ä¸‰å·ï¼Œå±…ä¸­
   - äºŒçº§æ ‡é¢˜ï¼ˆèŠ‚ï¼‰ï¼šé»‘ä½“å››å·ï¼Œå·¦å¯¹é½
   - ä¸‰çº§æ ‡é¢˜ï¼ˆå°èŠ‚ï¼‰ï¼šé»‘ä½“å°å››å·ï¼Œå·¦å¯¹é½

### Layer 3: ä¸­æ–‡å­¦æœ¯è¡¨è¾¾å®¡æŸ¥

**æ£€æŸ¥æ¸…å•**ï¼ˆè§ `docs/ACADEMIC_STYLE_ZH.md`ï¼‰ï¼š

#### A. å£è¯­åŒ–è¡¨è¾¾æ£€æµ‹
```latex
% é—®é¢˜ï¼ˆLine 23 in chap01.texï¼‰ï¼š
å¾ˆå¤šç ”ç©¶è¡¨æ˜æ·±åº¦å­¦ä¹ å¾ˆæœ‰ç”¨ã€‚

% åˆ†æï¼š
% - "å¾ˆå¤š" è¿‡äºå£è¯­åŒ–
% - "å¾ˆæœ‰ç”¨" è¡¨è¿°ä¸ç²¾ç¡®
% - ç¼ºå°‘å¼•ç”¨æ”¯æ’‘

% å»ºè®®æ”¹å†™ï¼š
å¤§é‡ç ”ç©¶è¡¨æ˜æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ \cite{lecun2015deep, krizhevsky2012imagenet}ã€‚

% æ”¹è¿›ç‚¹ï¼š
% 1. "å¾ˆå¤š" â†’ "å¤§é‡"ï¼ˆå­¦æœ¯åŒ–ï¼‰
% 2. "å¾ˆæœ‰ç”¨" â†’ "å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿"ï¼ˆç²¾ç¡®åŒ–ï¼‰
% 3. æ·»åŠ å¼•ç”¨æ”¯æ’‘ï¼ˆå­¦æœ¯è§„èŒƒï¼‰
```

#### B. ä¸»è§‚ç»å¯¹åŒ–è¯æ±‡
```latex
% âŒ ç¦ç”¨è¯æ±‡ï¼š
æ˜¾ç„¶ã€æ¯«æ— ç–‘é—®ã€ä¼—æ‰€å‘¨çŸ¥ã€å¿…ç„¶ã€ç»å¯¹ã€å®Œå…¨ã€æœ€å¥½ã€æœ€ä¼˜

% âœ… æ›¿ä»£è¡¨è¾¾ï¼š
æ˜¾ç„¶ â†’ ç ”ç©¶è¡¨æ˜ / å®éªŒç»“æœæ˜¾ç¤º
æ¯«æ— ç–‘é—® â†’ å¯ä»¥è®¤ä¸º / æœ‰ç†ç”±ç›¸ä¿¡
ä¼—æ‰€å‘¨çŸ¥ â†’ å·²æœ‰ç ”ç©¶æŒ‡å‡º / æ–‡çŒ®è¡¨æ˜
```

#### C. æœ¯è¯­ä¸€è‡´æ€§æ£€æŸ¥
```bash
python3 scripts/check_consistency.py main.tex --terms

# ç¤ºä¾‹è¾“å‡ºï¼š
# âš ï¸ æœ¯è¯­ä¸ä¸€è‡´ï¼ˆå…±17å¤„ï¼‰ï¼š
# - "æ·±åº¦å­¦ä¹ " (10æ¬¡) vs "æ·±åº¦ç¥ç»ç½‘ç»œ" (7æ¬¡)
# å»ºè®®ï¼šç»Ÿä¸€ä½¿ç”¨ "æ·±åº¦å­¦ä¹ "ï¼Œé¦–æ¬¡å‡ºç°æ—¶æ³¨é‡Šå…¨ç§°
#
# âš ï¸ ç¼©ç•¥è¯­ä¸ä¸€è‡´ï¼š
# - "CNN" é¦–æ¬¡å‡ºç°åœ¨ chap02.tex ç¬¬15è¡Œï¼Œä½†æœªç»™å‡ºå…¨ç§°
# å»ºè®®ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutional Neural Network, CNNï¼‰
```

#### D. æ ‡ç‚¹ç¬¦å·è§„èŒƒ
- ä¸­æ–‡æ ‡ç‚¹ï¼šï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘
- è‹±æ–‡æ ‡ç‚¹ï¼š, . ! ? ; : "" '' () []
- æ··ç”¨æ£€æµ‹ï¼šä¸­æ–‡å¥å­ä¸­çš„è‹±æ–‡å•è¯/æœ¯è¯­ååº”ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹

### Layer 4: é•¿éš¾å¥æ‹†è§£ï¼ˆä¸­æ–‡ç‰¹åŒ–ï¼‰

**è§¦å‘æ¡ä»¶**ï¼šå•å¥è¶…è¿‡ 60 å­—æˆ–åŒ…å« 3 ä¸ªä»¥ä¸Šä»å¥

```latex
% é•¿éš¾å¥æ£€æµ‹ï¼ˆchap03.tex ç¬¬45è¡Œï¼Œå…± 87 å­—ï¼‰
% åŸå¥ï¼š
æœ¬æ–‡æå‡ºçš„åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒåˆ†ç±»æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºæ¨¡å‹å¯¹å…³é”®åŒºåŸŸçš„å…³æ³¨ï¼Œå¹¶ç»“åˆæ•°æ®å¢å¼ºæŠ€æœ¯æ¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ã€‚

% ä¸»å¹²è¯†åˆ«ï¼š
æœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚

% ä¿®é¥°æˆåˆ†ï¼š
- [å®šè¯­] åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒåˆ†ç±»
- [æ–¹å¼] é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶
- [ç›®çš„] æ¥å¢å¼ºæ¨¡å‹å¯¹å…³é”®åŒºåŸŸçš„å…³æ³¨
- [å¹¶åˆ—] å¹¶ç»“åˆæ•°æ®å¢å¼ºæŠ€æœ¯
- [ç›®çš„] æ¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

% å»ºè®®æ”¹å†™ï¼ˆä¿å®ˆç‰ˆï¼‰ï¼š
æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒåˆ†ç±»æ–¹æ³•ã€‚
è¯¥æ–¹æ³•å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ä»¥å¢å¼ºæ¨¡å‹å¯¹å…³é”®åŒºåŸŸçš„å…³æ³¨ã€‚
åŒæ—¶ï¼Œç»“åˆæ•°æ®å¢å¼ºæŠ€æœ¯æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚
å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚

% å»ºè®®æ”¹å†™ï¼ˆå¢å¼ºé€»è¾‘ç‰ˆï¼‰ï¼š
é’ˆå¯¹ç°æœ‰å›¾åƒåˆ†ç±»æ–¹æ³•çš„ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ”¹è¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚
é¦–å…ˆï¼Œå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹èšç„¦äºå›¾åƒå…³é”®åŒºåŸŸã€‚
å…¶æ¬¡ï¼Œé‡‡ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯æå‡æ¨¡å‹é²æ£’æ€§ã€‚
å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ ImageNetã€CIFAR-10 ç­‰æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚
```

### Layer 5: è¡¨è¿°åˆ†æä¸é‡æ„

**è¾“å‡ºæ ¼å¼**ï¼ˆLaTeX æ³¨é‡Šå½¢å¼ï¼‰ï¼š
```latex
% â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
% ä¿®æ”¹å»ºè®®ï¼ˆchap02.tex ç¬¬ 23 è¡Œï¼‰
% â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
% é—®é¢˜ç±»å‹ï¼šè¡¨è¿°ä¸å¤Ÿå­¦æœ¯åŒ– + ç¼ºå°‘é€»è¾‘è¿æ¥
%
% åŸæ–‡ï¼š
æˆ‘ä»¬ä½¿ç”¨äº† ResNet æ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹æ•ˆæœå¾ˆå¥½ã€‚æˆ‘ä»¬åœ¨å®éªŒä¸­è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚

% ä¿®æ”¹åï¼ˆç†ç”±ï¼šæå‡å­¦æœ¯æ€§ + å¢å¼ºé€»è¾‘æ€§ï¼‰ï¼š
æœ¬æ–‡é‡‡ç”¨ ResNet æ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨ã€‚
è¯¥æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰² \cite{he2016deep}ã€‚
å®éªŒç»“æœéªŒè¯äº†è¯¥æ¨¡å‹åœ¨æœ¬æ–‡åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼ˆè¯¦è§ç¬¬ \ref{sec:experiments} èŠ‚ï¼‰ã€‚

% æ”¹è¿›ç‚¹ï¼š
% 1. "æˆ‘ä»¬ä½¿ç”¨" â†’ "æœ¬æ–‡é‡‡ç”¨"ï¼ˆå­¦æœ¯è§„èŒƒï¼‰
% 2. "è¿™ä¸ªæ¨¡å‹" â†’ "è¯¥æ¨¡å‹"ï¼ˆé¿å…å£è¯­åŒ–ï¼‰
% 3. "æ•ˆæœå¾ˆå¥½" â†’ "è¡¨ç°å‡ºè‰²"ï¼ˆç²¾ç¡®åŒ–ï¼‰+ æ·»åŠ å¼•ç”¨
% 4. å¢åŠ ç« èŠ‚äº¤å‰å¼•ç”¨ï¼ˆé€»è¾‘è¿è´¯ï¼‰
% â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## å­¦æ ¡æ¨¡æ¿é€‚é…

**è‡ªåŠ¨æ£€æµ‹**ï¼š
```bash
# æ£€æµ‹æ¨¡æ¿ç±»å‹
grep "documentclass" main.tex

# åŠ è½½å¯¹åº”è§„åˆ™
if [[ $template == "thuthesis" ]]; then
    source docs/UNIVERSITIES/tsinghua.md
elif [[ $template == "pkuthss" ]]; then
    source docs/UNIVERSITIES/pku.md
else
    source docs/UNIVERSITIES/generic.md
fi
```

**æ¨¡æ¿å·®å¼‚ç¤ºä¾‹**ï¼ˆè§ `docs/UNIVERSITIES/`ï¼‰ï¼š
- **æ¸…å**ï¼šå›¾è¡¨ç¼–å·æ ¼å¼ `å›¾ 3-1`ï¼Œå‚è€ƒæ–‡çŒ®ç”¨ `thubib.bst`
- **åŒ—å¤§**ï¼šå›¾è¡¨ç¼–å·æ ¼å¼ `å›¾3.1`ï¼Œè¦æ±‚å•ç‹¬çš„"ç¬¦å·è¯´æ˜"ç« èŠ‚
- **é€šç”¨**ï¼šéµå¾ªå›½æ ‡åŸºæœ¬è¦æ±‚

## è¾“å‡ºæŠ¥å‘Šç»“æ„

```markdown
# LaTeX åšå£«è®ºæ–‡å®¡æŸ¥æŠ¥å‘Š

## ğŸ“Š æ€»è§ˆ
- æ•´ä½“çŠ¶æ€ï¼šâœ… ç¬¦åˆè¦æ±‚ / âš ï¸ éœ€è¦ä¿®è®¢ / âŒ é‡å¤§é—®é¢˜
- ç¼–è¯‘çŠ¶æ€ï¼š[chktex ç»“æœ]
- æ–‡ä»¶ç»“æ„ï¼š[map_structure ç»“æœ]
- æ¨¡æ¿ç±»å‹ï¼šæ¸…åå¤§å­¦åšå£«è®ºæ–‡æ¨¡æ¿ï¼ˆthuthesis v7.2ï¼‰

## ğŸ“ ç»“æ„å®Œæ•´æ€§ï¼ˆ8/10 é€šè¿‡ï¼‰
### âœ… å·²å®Œæˆé¡¹
- å°é¢ã€å£°æ˜ã€ä¸­è‹±æ–‡æ‘˜è¦ã€ç›®å½•ã€æ­£æ–‡ã€å‚è€ƒæ–‡çŒ®ã€è‡´è°¢

### âš ï¸ å¾…å®Œå–„é¡¹
- ç¬¦å·å¯¹ç…§è¡¨ï¼ˆå»ºè®®æ·»åŠ ï¼‰
- æ”»è¯»å­¦ä½æœŸé—´å‘è¡¨è®ºæ–‡ç›®å½•ï¼ˆç¼ºå°‘ä¼šè®®è®ºæ–‡ C1ï¼‰

## ğŸ“ å›½æ ‡æ ¼å¼å®¡æŸ¥
### âœ… ç¬¦åˆé¡¹
- å‚è€ƒæ–‡çŒ®ä½¿ç”¨ biblatex-gb7714-2015
- å›¾è¡¨é¢˜æ³¨ä½ç½®æ­£ç¡®

### âŒ ä¸ç¬¦åˆé¡¹
- [ç¬¬ 15 æ¡å‚è€ƒæ–‡çŒ®] ç¼ºå°‘ DOI
- [å›¾ 3.2] é¢˜æ³¨å­—ä½“åº”ä¸ºå®‹ä½“äº”å·ï¼ˆå½“å‰ä¸ºé»‘ä½“ï¼‰

## ğŸ“ ä¸­æ–‡å­¦æœ¯è¡¨è¾¾ï¼ˆ12 å¤„å»ºè®®ï¼‰
### é«˜ä¼˜å…ˆçº§
- [chap01.tex:23] å£è¯­åŒ–ï¼š"å¾ˆå¤šç ”ç©¶" â†’ "å¤§é‡ç ”ç©¶"
- [chap02.tex:45] ä¸»è§‚åŒ–ï¼š"æ˜¾ç„¶" â†’ "ç ”ç©¶è¡¨æ˜"

### ä¸­ä¼˜å…ˆçº§
- [chap03.tex:67] æœ¯è¯­ä¸ä¸€è‡´ï¼šç»Ÿä¸€ä½¿ç”¨"æ·±åº¦å­¦ä¹ "

## ğŸ“ é•¿éš¾å¥æ‹†è§£ï¼ˆ3 å¤„ï¼‰
[æ¯ä¸ªé•¿éš¾å¥çš„è¯¦ç»†åˆ†æ]

## ğŸ¨ è¡¨è¿°é‡æ„å»ºè®®ï¼ˆ5 å¤„ï¼‰
[æŒ‰ç« èŠ‚åˆ†ç»„çš„æ”¹å†™å»ºè®®]

## âœ… ä¸‹ä¸€æ­¥è¡ŒåŠ¨
1. ä¿®å¤ 2 ä¸ªå›½æ ‡æ ¼å¼é—®é¢˜ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
2. å¤„ç† 12 ä¸ªå­¦æœ¯è¡¨è¾¾å»ºè®®ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰
3. è€ƒè™‘æ‹†è§£ 3 ä¸ªé•¿éš¾å¥ï¼ˆä½ä¼˜å…ˆçº§ï¼‰
4. è¡¥å……ç¬¦å·å¯¹ç…§è¡¨å’Œè®ºæ–‡ç›®å½•ï¼ˆå®Œå–„é¡¹ï¼‰
```

## å·¥å…·ä½¿ç”¨

```bash
# ç»“æ„æ˜ å°„ï¼ˆå¿…é¡»é¦–å…ˆæ‰§è¡Œï¼‰
python3 scripts/map_structure.py main.tex

# æ ¼å¼æ£€æŸ¥
bash scripts/check_format.sh main.tex

# ç»“æ„éªŒè¯
python3 scripts/verify_structure.py main.tex --template tsinghua

# æœ¯è¯­ä¸€è‡´æ€§
python3 scripts/check_consistency.py main.tex --terms

# å­—æ•°ç»Ÿè®¡ï¼ˆæŒ‰ç« èŠ‚ï¼‰
python3 scripts/count_words.py main.tex --by-chapter

# æŸ¥æ‰¾æœªå®Œæˆå†…å®¹
grep -rn "TODO\|FIXME\|XXX\|å¾…å®Œæˆ" data/
```

## å®‰å…¨æœºåˆ¶

1. **åªè¯»ä¼˜å…ˆ**ï¼šé»˜è®¤åªæä¾›å»ºè®®ï¼Œä¸è‡ªåŠ¨ä¿®æ”¹
2. **æ³¨é‡Šå½¢å¼è¾“å‡º**ï¼šæ‰€æœ‰æ”¹åŠ¨ä»¥ LaTeX æ³¨é‡Šå‘ˆç°ï¼Œä¾¿äºå®¡æŸ¥
3. **æœ¯è¯­ä¿æŠ¤**ï¼šåŠ è½½ FORBIDDEN_CHANGES.md ä¸­çš„ä¿æŠ¤åˆ—è¡¨
4. **å¼•ç”¨å®Œæ•´æ€§**ï¼šéªŒè¯æ‰€æœ‰ `\cite{}` åœ¨ .bib ä¸­å­˜åœ¨
5. **æ•°å­¦ç¯å¢ƒä¿æŠ¤**ï¼šç»ä¸è§£æå…¬å¼å†…å®¹
6. **åˆ†ç« èŠ‚å¤„ç†**ï¼šå¤§æ–‡ä»¶æŒ‰ç« èŠ‚é€ä¸ªå¤„ç†ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡ä¸¢å¤±

## é”™è¯¯å¤„ç†

```python
# å¦‚æœ map_structure.py å¤±è´¥
if not can_run("python3 scripts/map_structure.py"):
    print("âš ï¸ æ— æ³•æ˜ å°„è®ºæ–‡ç»“æ„")
    print("ğŸ“ å°†å°è¯•æ‰‹åŠ¨æŸ¥æ‰¾ \\input å’Œ \\include å‘½ä»¤")
    # é™çº§åˆ°æ‰‹åŠ¨æ‰«ææ¨¡å¼

# å¦‚æœ chktex ä¸å¯ç”¨
if not command_exists("chktex"):
    print("âš ï¸ chktex æœªå®‰è£…ï¼Œå»ºè®®å®‰è£…ï¼šsudo apt-get install chktex")
    print("ğŸ“ é™çº§ä¸ºåŸºäº LLM çš„æ ¼å¼æ£€æŸ¥ï¼ˆç²¾åº¦è¾ƒä½ï¼‰")
```

## æ¸è¿›å¼åŠ è½½

ä»é«˜å±‚æ‘˜è¦å¼€å§‹ï¼Œåªåœ¨éœ€è¦<invoke name="artifacts">
<parameter name="command">create</parameter>
<parameter name="id">core_scripts</parameter>
<parameter name="title">æ ¸å¿ƒè„šæœ¬å®ç°</parameter>
<parameter name="type">application/vnd.ant.code</parameter>
<parameter name="language">python</parameter>
<parameter name="content">#!/usr/bin/env python3
"""
LaTeX å­¦æœ¯å†™ä½œ Skills æ ¸å¿ƒè„šæœ¬é›†åˆ
"""

# ============================================================================
# 1. map_structure.py - è®ºæ–‡ç»“æ„æ˜ å°„å™¨ï¼ˆä¸­æ–‡åšå£«è®ºæ–‡æ ¸å¿ƒè„šæœ¬ï¼‰
# ============================================================================

import re
import sys
from pathlib import Path
from typing import List, Dict, Set

class ThesisStructureMapper:
    """è§£æ LaTeX è®ºæ–‡çš„æ–‡ä»¶ä¾èµ–æ ‘"""
    
    def __init__(self, main_file: str):
        self.main_file = Path(main_file)
        self.root_dir = self.main_file.parent
        self.structure = []
        self.visited = set()
        
    def parse_file(self, tex_file: Path, level: int = 0) -> List[Dict]:
        """é€’å½’è§£æ LaTeX æ–‡ä»¶"""
        if tex_file in self.visited:
            return []
        self.visited.add(tex_file)
        
        if not tex_file.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {tex_file}")
            return []
            
        result = [{
            'file': str(tex_file.relative_to(self.root_dir)),
            'level': level,
            'type': self._detect_type(tex_file)
        }]
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            content = tex_file.read_text(encoding='utf-8', errors='ignore')
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è¯»å– {tex_file}: {e}")
            return result
        
        # æŸ¥æ‰¾ \input{} å’Œ \include{}
        patterns = [
            r'\\input\{([^}]+)\}',
            r'\\include\{([^}]+)\}'
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, content):
                included = match.group(1)
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if not included.endswith('.tex'):
                    included += '.tex'
                included_path = (self.root_dir / included).resolve()
                result.extend(self.parse_file(included_path, level + 1))
                
        return result
    
    def _detect_type(self, tex_file: Path) -> str:
        """æ£€æµ‹æ–‡ä»¶ç±»å‹"""
        name = tex_file.stem.lower()
        if 'cover' in name or 'titlepage' in name:
            return 'å°é¢'
        elif 'abstract' in name or 'abs' in name:
            return 'æ‘˜è¦'
        elif 'chap' in name or 'chapter' in name:
            match = re.search(r'(\d+)', name)
            if match:
                return f'ç¬¬{match.group(1)}ç« '
            return 'ç« èŠ‚'
        elif 'ref' in name or 'bib' in name:
            return 'å‚è€ƒæ–‡çŒ®'
        elif 'appendix' in name:
            return 'é™„å½•'
        elif 'ack' in name or 'thanks' in name:
            return 'è‡´è°¢'
        else:
            return 'å…¶ä»–'
    
    def generate_tree(self) -> str:
        """ç”Ÿæˆæ ‘å½¢ç»“æ„å­—ç¬¦ä¸²"""
        structure = self.parse_file(self.main_file)
        
        lines = []
        lines.append(f"\nğŸ“ è®ºæ–‡ç»“æ„æ ‘:")
        lines.append("=" * 60)
        
        for item in structure:
            indent = '  ' * item['level']
            prefix = 'â”œâ”€â”€ ' if item['level'] > 0 else ''
            lines.append(f"{indent}{prefix}{item['file']} ({item['type']})")
        
        lines.append("=" * 60)
        return '\n'.join(lines)
    
    def get_processing_order(self) -> List[str]:
        """è·å–å»ºè®®çš„å¤„ç†é¡ºåº"""
        structure = self.parse_file(self.main_file)
        
        # æŒ‰ç±»å‹ä¼˜å…ˆçº§æ’åº
        priority = {
            'å°é¢': 0,
            'æ‘˜è¦': 1,
            'ç¬¬1ç« ': 2,
            'ç¬¬2ç« ': 3,
            'ç« èŠ‚': 4,
            'å‚è€ƒæ–‡çŒ®': 5,
            'è‡´è°¢': 6,
            'é™„å½•': 7,
            'å…¶ä»–': 8
        }
        
        def get_priority(item):
            for key in priority:
                if item['type'].startswith(key):
                    return priority[key]
            return priority['å…¶ä»–']
        
        sorted_structure = sorted(structure, key=get_priority)
        return [item['file'] for item in sorted_structure]


# ============================================================================
# 2. extract_prose.py - çº¯æ–‡æœ¬æå–å™¨
# ============================================================================

class LaTeXProseExtractor:
    """ä» LaTeX æ–‡ä»¶ä¸­æå–çº¯æ–‡æœ¬ï¼ˆè·³è¿‡å…¬å¼ã€å¼•ç”¨ã€å®ï¼‰"""
    
    def __init__(self):
        # éœ€è¦è·³è¿‡çš„ç¯å¢ƒ
        self.skip_environments = [
            'equation', 'align', 'gather', 'multline',
            'eqnarray', 'displaymath', 'figure', 'table'
        ]
        
    def extract(self, tex_file: Path) -> str:
        """æå–çº¯æ–‡æœ¬"""
        content = tex_file.read_text(encoding='utf-8', errors='ignore')
        
        # ç§»é™¤æ³¨é‡Š
        content = re.sub(r'(?<!\\)%.*', '', content)
        
        # ç§»é™¤æ•°å­¦ç¯å¢ƒ
        for env in self.skip_environments:
            pattern = rf'\\begin\{{{env}\}}.*?\\end\{{{env}\}}'
            content = re.sub(pattern, '', content, flags=re.DOTALL)
        
        # ç§»é™¤è¡Œå†…å…¬å¼
        content = re.sub(r'\$[^$]+\$', '', content)
        content = re.sub(r'\\\[.*?\\\]', '', content, flags=re.DOTALL)
        content = re.sub(r'\\\(.*?\\\)', '', content, flags=re.DOTALL)
        
        # ç§»é™¤ LaTeX å‘½ä»¤ä½†ä¿ç•™å†…å®¹
        # \cite{}, \ref{}, \label{}
        content = re.sub(r'\\cite\{[^}]+\}', '[CITE]', content)
        content = re.sub(r'\\ref\{[^}]+\}', '[REF]', content)
        content = re.sub(r'\\label\{[^}]+\}', '', content)
        
        # \textbf{}, \textit{}, \emph{}
        content = re.sub(r'\\(?:textbf|textit|emph)\{([^}]+)\}', r'\1', content)
        
        # ç§»é™¤å…¶ä»–å‘½ä»¤
        content = re.sub(r'\\[a-zA-Z]+\*?(\[.*?\])?(\{[^}]*\})*', '', content)
        
        # æ¸…ç†ç©ºç™½
        content = re.sub(r'\n\s*\n', '\n\n', content)
        content = re.sub(r' +', ' ', content)
        
        return content.strip()


# ============================================================================
# 3. analyze_sentence.py - é•¿éš¾å¥åˆ†æå™¨
# ============================================================================

class SentenceAnalyzer:
    """åˆ†æå¥å­å¤æ‚åº¦"""
    
    def __init__(self, lang='en'):
        self.lang = lang
        self.word_threshold = 50 if lang == 'en' else 60
        self.char_threshold = 200 if lang == 'zh' else 300
        
    def analyze(self, text: str) -> List[Dict]:
        """åˆ†ææ–‡æœ¬ä¸­çš„å¤æ‚å¥å­"""
        sentences = self._split_sentences(text)
        complex_sentences = []
        
        for i, sent in enumerate(sentences, 1):
            analysis = self._analyze_sentence(sent, i)
            if analysis['is_complex']:
                complex_sentences.append(analysis)
        
        return complex_sentences
    
    def _split_sentences(self, text: str) -> List[str]:
        """æ‹†åˆ†å¥å­"""
        if self.lang == 'zh':
            # ä¸­æ–‡ï¼šæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·æ‹†åˆ†
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        else:
            # è‹±æ–‡ï¼šæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·æ‹†åˆ†ï¼ˆè€ƒè™‘ç¼©å†™ï¼‰
            sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', text)
        
        return [s.strip() for s in sentences if s.strip()]
    
    def _analyze_sentence(self, sent: str, index: int) -> Dict:
        """åˆ†æå•ä¸ªå¥å­"""
        if self.lang == 'en':
            word_count = len(sent.split())
            is_complex = word_count > self.word_threshold
            
            # ç»Ÿè®¡ä»å¥æ•°é‡ï¼ˆç®€å•å¯å‘å¼ï¼‰
            subordinate_markers = [
                'which', 'that', 'when', 'where', 'who', 'whose',
                'although', 'because', 'since', 'unless', 'while'
            ]
            subordinate_count = sum(
                sent.lower().count(f' {marker} ') 
                for marker in subordinate_markers
            )
        else:
            char_count = len(sent)
            is_complex = char_count > self.char_threshold
            
            # ç»Ÿè®¡ä¸­æ–‡ä»å¥æ ‡è®°
            subordinate_markers = ['ï¼Œ', 'ï¼›', 'ã€']
            subordinate_count = sum(sent.count(marker) for marker in subordinate_markers)
            word_count = char_count
        
        return {
            'index': index,
            'sentence': sent,
            'word_count': word_count,
            'subordinate_count': subordinate_count,
            'is_complex': is_complex or subordinate_count >= 3
        }


# ============================================================================
# 4. verify_bib.py - BibTeX å®Œæ•´æ€§æ£€æŸ¥
# ============================================================================

class BibTeXVerifier:
    """éªŒè¯ BibTeX æ–‡ä»¶çš„å®Œæ•´æ€§"""
    
    REQUIRED_FIELDS = {
        'article': ['author', 'title', 'journal', 'year'],
        'inproceedings': ['author', 'title', 'booktitle', 'year'],
        'book': ['author', 'title', 'publisher', 'year'],
        'phdthesis': ['author', 'title', 'school', 'year'],
    }
    
    def verify(self, bib_file: Path, standard='default') -> Dict:
        """éªŒè¯ BibTeX æ–‡ä»¶"""
        content = bib_file.read_text(encoding='utf-8', errors='ignore')
        
        # è§£ææ¡ç›®
        entries = self._parse_entries(content)
        
        issues = []
        for entry in entries:
            entry_type = entry['type']
            entry_key = entry['key']
            fields = entry['fields']
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if entry_type in self.REQUIRED_FIELDS:
                required = self.REQUIRED_FIELDS[entry_type]
                missing = [f for f in required if f not in fields]
                
                if missing:
                    issues.append({
                        'key': entry_key,
                        'type': 'missing_field',
                        'missing': missing
                    })
            
            # æ£€æŸ¥ DOIï¼ˆå›½æ ‡æ¨èï¼‰
            if standard == 'gb7714' and 'doi' not in fields:
                issues.append({
                    'key': entry_key,
                    'type': 'missing_doi',
                    'message': 'å»ºè®®æ·»åŠ  DOIï¼ˆå›½æ ‡æ¨èï¼‰'
                })
        
        return {
            'total_entries': len(entries),
            'issues': issues,
            'status': 'PASS' if not issues else 'WARNING'
        }
    
    def _parse_entries(self, content: str) -> List[Dict]:
        """è§£æ BibTeX æ¡ç›®ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        entries = []
        pattern = r'@(\w+)\{([^,]+),\s*(.*?)\n\}'
        
        for match in re.finditer(pattern, content, re.DOTALL):
            entry_type = match.group(1).lower()
            entry_key = match.group(2).strip()
            fields_str = match.group(3)
            
            fields = {}
            field_pattern = r'(\w+)\s*=\s*\{([^}]*)\}'
            for field_match in re.finditer(field_pattern, fields_str):
                field_name = field_match.group(1).lower()
                field_value = field_match.group(2)
                fields[field_name] = field_value
            
            entries.append({
                'type': entry_type,
                'key': entry_key,
                'fields': fields
            })
        
        return entries


# ============================================================================
# 5. check_consistency.py - æœ¯è¯­ä¸€è‡´æ€§æ£€æŸ¥å™¨
# ============================================================================

class TermConsistencyChecker:
    """æ£€æŸ¥æœ¯è¯­ä½¿ç”¨çš„ä¸€è‡´æ€§"""
    
    def check(self, tex_files: List[Path]) -> Dict:
        """æ£€æŸ¥æœ¯è¯­ä¸€è‡´æ€§"""
        term_occurrences = {}
        
        for tex_file in tex_files:
            content = tex_file.read_text(encoding='utf-8', errors='ignore')
            
            # æå–æœ¯è¯­ï¼ˆç®€åŒ–ï¼šæŸ¥æ‰¾é¦–å­—æ¯å¤§å†™çš„ä¸“ä¸šæœ¯è¯­ï¼‰
            # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„ NLP å¤„ç†
            terms = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', content)
            
            for term in terms:
                if term not in term_occurrences:
                    term_occurrences[term] = []
                term_occurrences[term].append(tex_file.name)
        
        # æŸ¥æ‰¾å¯èƒ½çš„ä¸ä¸€è‡´ï¼ˆç›¸ä¼¼æœ¯è¯­ï¼‰
        inconsistencies = []
        checked = set()
        
        for term1 in term_occurrences:
            if term1 in checked:
                continue
            
            for term2 in term_occurrences:
                if term1 != term2 and term2 not in checked:
                    # ç®€å•ç›¸ä¼¼åº¦ï¼šç¼–è¾‘è·ç¦»
                    if self._is_similar(term1, term2):
                        inconsistencies.append({
                            'term1': term1,
                            'count1': len(term_occurrences[term1]),
                            'term2': term2,
                            'count2': len(term_occurrences[term2]),
                            'suggestion': f"ç»Ÿä¸€ä½¿ç”¨ '{term1}'" if len(term_occurrences[term1]) > len(term_occurrences[term2]) else f"ç»Ÿä¸€ä½¿ç”¨ '{term2}'"
                        })
                        checked.add(term1)
                        checked.add(term2)
        
        return {
            'total_terms': len(term_occurrences),
            'inconsistencies': inconsistencies
        }
    
    def _is_similar(self, term1: str, term2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæœ¯è¯­æ˜¯å¦ç›¸ä¼¼ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ç¼–è¾‘è·ç¦»æˆ–å…¶ä»– NLP æ–¹æ³•
        words1 = set(term1.lower().split())
        words2 = set(term2.lower().split())
        
        # å¦‚æœæœ‰å…±åŒè¯æ±‡ä¸”é•¿åº¦ç›¸è¿‘
        common = words1 & words2
        return len(common) > 0 and abs(len(words1) - len(words2)) <= 1


# ============================================================================
# 6. make_diff.py - Diff ç”Ÿæˆå™¨ï¼ˆLaTeX æ³¨é‡Šå½¢å¼ï¼‰
# ============================================================================

class LaTeXDiffGenerator:
    """ç”Ÿæˆ LaTeX æ³¨é‡Šå½¢å¼çš„ Diff"""
    
    def generate(self, original: str, modified: str, reason: str, line_num: int = None) -> str:
        """ç”Ÿæˆ diff æ³¨é‡Š"""
        if line_num:
            header = f"% {'â•' * 60}\n% ä¿®æ”¹å»ºè®®ï¼ˆç¬¬ {line_num} è¡Œï¼‰\n% {'â•' * 60}"
        else:
            header = f"% {'â•' * 60}\n% ä¿®æ”¹å»ºè®®\n% {'â•' * 60}"
        
        diff = f"""
{header}
% é—®é¢˜ç±»å‹ï¼š{reason}
%
% åŸæ–‡ï¼š
% {original}
%
% ä¿®æ”¹åï¼š
{modified}
%
% æ”¹è¿›ç‚¹ï¼š
% [åœ¨è¿™é‡Œåˆ—å‡ºå…·ä½“æ”¹è¿›]
% {'â•' * 60}
"""
        return diff
    
    def generate_batch(self, changes: List[Dict]) -> str:
        """æ‰¹é‡ç”Ÿæˆ diff"""
        all_diffs = []
        
        for change in changes:
            diff = self.generate(
                original=change['original'],
                modified=change['modified'],
                reason=change['reason'],
                line_num=change.get('line_num')
            )
            all_diffs.append(diff)
        
        return '\n\n'.join(all_diffs)


# ============================================================================
# ä¸»å‡½æ•°ç¤ºä¾‹
# ============================================================================

if __name__ == '__main__':
    # ç¤ºä¾‹ 1: æ˜ å°„è®ºæ–‡ç»“æ„
    if len(sys.argv) > 1 and sys.argv[1] == 'map':
        mapper = ThesisStructureMapper('main.tex')
        print(mapper.generate_tree())
        print("\nå»ºè®®å¤„ç†é¡ºåº:")
        for i, file in enumerate(mapper.get_processing_order(), 1):
            print(f"  {i}. {file}")
    
    # ç¤ºä¾‹ 2: æå–çº¯æ–‡æœ¬
    elif len(sys.argv) > 1 and sys.argv[1] == 'extract':
        extractor = LaTeXProseExtractor()
        prose = extractor.extract(Path('main.tex'))
        print(prose)
    
    # ç¤ºä¾‹ 3: åˆ†æå¥å­
    elif len(sys.argv) > 1 and sys.argv[1] == 'analyze':
        analyzer = SentenceAnalyzer(lang='en')
        text = "This is a sample text. This is a very long sentence with multiple clauses that should be flagged as complex because it contains too many words."
        results = analyzer.analyze(text)
        for result in results:
            print(f"å¥å­ {result['index']}: {result['word_count']} è¯, {result['subordinate_count']} ä»å¥")
    
    else:
        print("ç”¨æ³•:")
        print("  python scripts.py map       - æ˜ å°„è®ºæ–‡ç»“æ„")
        print("  python scripts.py extract   - æå–çº¯æ–‡æœ¬")
        print("  python scripts.py analyze   - åˆ†æå¥å­")
</parameter>